{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmFU4cvyIl1obYC8G5mPRL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pety99/food101/blob/main/food101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FymY9c7MB5M3",
        "outputId": "9b8c5ddf-f6fa-49a9-90d9-7ecc431ca7ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "!pip install -U tensorflow_datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.24.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (20.2.0)\n",
            "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.3.2)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.52.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow_datasets) (50.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MZpztT9DHjV"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHvLiqg7DKgF"
      },
      "source": [
        "# Import TensorFlow Datasets\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "# Helper libraries\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n97f96hAFC1s"
      },
      "source": [
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm8u1IGzYNPD"
      },
      "source": [
        "# Adathaémaz letöltése a TensorFlow Datasets-ről\n",
        "A [food101](https://www.tensorflow.org/datasets/catalog/food101) adathalmazt fogjuk használni, amiben 101000 kép található 101 külöböző kategóriába tartozó ételről, kategóriánként 1000 képpel. Az adathalmazt elszeparáltuk egy train és egy validációs halmazra.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvn7JibtFcBF"
      },
      "source": [
        "dataset, metadata = tfds.load('food101', as_supervised=True, with_info=True)\n",
        "train_dataset, validation_dataset = dataset['train'], dataset['validation']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obfBnvA3H_e6",
        "outputId": "6f8524fb-7907-4bca-c6a5-8685942d626e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "num_classes = metadata.features['label'].num_classes\n",
        "num_train_examples = metadata.splits['train'].num_examples\n",
        "num_val_examples = metadata.splits['validation'].num_examples\n",
        "\n",
        "\n",
        "print('Total Number of Classes: {}'.format(num_classes))\n",
        "print(\"Number of training examples: {}\".format(num_train_examples))\n",
        "print(\"Number of validation examples:     {}\".format(num_val_examples))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of Classes: 101\n",
            "Number of training examples: 75750\n",
            "Number of validation examples:     25250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLVG3iXqPnRd",
        "outputId": "cbc9cf12-c8ac-4ff9-d776-e605538d44bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "for i, example in enumerate(train_dataset.take(5)):\n",
        "  print('Image {} shape: {} label: {}'.format(i+1, example[0].shape, example[1]))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image 1 shape: (512, 512, 3) label: 56\n",
            "Image 2 shape: (384, 512, 3) label: 76\n",
            "Image 3 shape: (512, 512, 3) label: 21\n",
            "Image 4 shape: (512, 512, 3) label: 64\n",
            "Image 5 shape: (512, 512, 3) label: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rff6soLEQoVB",
        "outputId": "839d2639-2368-488f-9707-7497a75533e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "class_names = np.array(metadata.features['label'].names)\n",
        "\n",
        "print(class_names)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['apple_pie' 'baby_back_ribs' 'baklava' 'beef_carpaccio' 'beef_tartare'\n",
            " 'beet_salad' 'beignets' 'bibimbap' 'bread_pudding' 'breakfast_burrito'\n",
            " 'bruschetta' 'caesar_salad' 'cannoli' 'caprese_salad' 'carrot_cake'\n",
            " 'ceviche' 'cheesecake' 'cheese_plate' 'chicken_curry'\n",
            " 'chicken_quesadilla' 'chicken_wings' 'chocolate_cake' 'chocolate_mousse'\n",
            " 'churros' 'clam_chowder' 'club_sandwich' 'crab_cakes' 'creme_brulee'\n",
            " 'croque_madame' 'cup_cakes' 'deviled_eggs' 'donuts' 'dumplings' 'edamame'\n",
            " 'eggs_benedict' 'escargots' 'falafel' 'filet_mignon' 'fish_and_chips'\n",
            " 'foie_gras' 'french_fries' 'french_onion_soup' 'french_toast'\n",
            " 'fried_calamari' 'fried_rice' 'frozen_yogurt' 'garlic_bread' 'gnocchi'\n",
            " 'greek_salad' 'grilled_cheese_sandwich' 'grilled_salmon' 'guacamole'\n",
            " 'gyoza' 'hamburger' 'hot_and_sour_soup' 'hot_dog' 'huevos_rancheros'\n",
            " 'hummus' 'ice_cream' 'lasagna' 'lobster_bisque' 'lobster_roll_sandwich'\n",
            " 'macaroni_and_cheese' 'macarons' 'miso_soup' 'mussels' 'nachos'\n",
            " 'omelette' 'onion_rings' 'oysters' 'pad_thai' 'paella' 'pancakes'\n",
            " 'panna_cotta' 'peking_duck' 'pho' 'pizza' 'pork_chop' 'poutine'\n",
            " 'prime_rib' 'pulled_pork_sandwich' 'ramen' 'ravioli' 'red_velvet_cake'\n",
            " 'risotto' 'samosa' 'sashimi' 'scallops' 'seaweed_salad'\n",
            " 'shrimp_and_grits' 'spaghetti_bolognese' 'spaghetti_carbonara'\n",
            " 'spring_rolls' 'steak' 'strawberry_shortcake' 'sushi' 'tacos' 'takoyaki'\n",
            " 'tiramisu' 'tuna_tartare' 'waffles']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK6QMgGdRlte"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}